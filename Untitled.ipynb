{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gensim.models import KeyedVectors\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import torchtext\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import random\n",
    "from data_handling import *\n",
    "\n",
    "from create_vocab import *\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## our shit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset('data/en_gum-ud-train-projectivized.conllu')\n",
    "dev_data = Dataset('data/en_gum-ud-dev-projectivized.conllu')\n",
    "test_data = Dataset('data/en_gum-ud-test-projectivized.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#filename='GoogleNews-vectors-negative300.bin' ; binary = False ; no_header = False# ~ 80% accuracy\n",
    "\n",
    "#filename = 'enwiki_20180420_win10_100d.txt' ; binary = False  ; no_header = False # 85% accuracy\n",
    "#filename = 'enwiki_20180420_win10_300d.txt' ; binary = False ; no_header = False\n",
    "#filename = 'enwiki_20180420_win10_500d.txt' ; binary = False ; no_header = False\n",
    "#filename = 'enwiki_20180420_nolg_300d.txt' ; binary = False ; no_header = False\n",
    "#filename = 'enwiki_20180420_nolg_500d.txt' ; binary = False ; no_header = False\n",
    "\n",
    "#wikipedia + gigaword 5\n",
    "#filename = 'glove.6B.50d.txt' ; binary = False ; no_header = True\n",
    "#filename = 'glove.6B.100d.txt' ; binary = False ; no_header = True  # 89.21% \n",
    "#filename = 'glove.6B.200d.txt' ; binary = False ; no_header = True # ~88.4% small hidden size\n",
    "#filename = 'glove.6B.300d.txt' ; binary = False ; no_header = True\n",
    "\n",
    "\n",
    "# Twitter embeddings\n",
    "#filename = 'glove.twitter.27B.25d.txt' ; binary = False ; no_header = True\n",
    "#filename = 'glove.twitter.27B.50d.txt' ; binary = False ; no_header = True\n",
    "#filename = 'glove.twitter.27B.100d.txt' ; binary = False ; no_header = True\n",
    "#filename = 'glove.twitter.27B.200d.txt' ; binary = False ; no_header = True\n",
    "\n",
    "\n",
    "\n",
    "filename = 'glove.840B.300d.txt' ; binary = False ; no_header = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#word2vec_model = KeyedVectors.load_word2vec_format(filename, binary=binary, no_header=no_header)\n",
    "word2vec_model = KeyedVectors.load('test_model')\n",
    "word2vec_weights = torch.FloatTensor(word2vec_model.vectors)\n",
    "\n",
    "def training_examples_tagger2(vocab_words, vocab_tags, gold_data, batch_size=100, max_len=40):\n",
    "    assert batch_size > 0 and max_len > 0\n",
    "        \n",
    "    # max sequence length\n",
    "    x = torch.zeros((batch_size, max_len)).long()\n",
    "    y = torch.zeros((batch_size, max_len)).long()\n",
    "    count = 0\n",
    "    for sentence in gold_data:\n",
    "        words = torch.Tensor(list(map(lambda x: word2vec_model.get_index(x[0]) if x[0] in word2vec_model else 1, sentence))).long()\n",
    "        \n",
    "        labels = torch.Tensor(list(map(lambda x: vocab_tags[x[1]], sentence))).long()\n",
    "        # pad to max len\n",
    "        x[count,:len(words)] = words[:max_len]\n",
    "        y[count,:len(labels)] = labels[:max_len]\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        if(count == batch_size):\n",
    "            yield x.long() ,y.long()\n",
    "            x = torch.zeros((batch_size, max_len))\n",
    "            y = torch.zeros((batch_size, max_len))\n",
    "            count = 0\n",
    "    if(count):\n",
    "        yield x[:count,:].long(), y[:count,:].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 2.679 | Train Acc: 15.05%\n",
      "\t Val. Loss: 2.557 |  Val. Acc: 17.47%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 2.516 | Train Acc: 18.56%\n",
      "\t Val. Loss: 2.397 |  Val. Acc: 21.36%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 2.275 | Train Acc: 28.67%\n",
      "\t Val. Loss: 2.096 |  Val. Acc: 37.82%\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 1.943 | Train Acc: 41.30%\n",
      "\t Val. Loss: 1.777 |  Val. Acc: 46.14%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 1.703 | Train Acc: 47.88%\n",
      "\t Val. Loss: 1.591 |  Val. Acc: 49.34%\n",
      "Epoch: 06 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 1.514 | Train Acc: 53.05%\n",
      "\t Val. Loss: 1.383 |  Val. Acc: 56.82%\n",
      "Epoch: 07 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 1.351 | Train Acc: 57.13%\n",
      "\t Val. Loss: 1.243 |  Val. Acc: 60.15%\n",
      "Epoch: 08 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 1.238 | Train Acc: 59.84%\n",
      "\t Val. Loss: 1.143 |  Val. Acc: 63.23%\n",
      "Epoch: 09 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 1.154 | Train Acc: 62.37%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 65.70%\n",
      "Epoch: 10 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 1.085 | Train Acc: 64.99%\n",
      "\t Val. Loss: 1.004 |  Val. Acc: 66.90%\n",
      "Epoch: 11 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 1.019 | Train Acc: 67.58%\n",
      "\t Val. Loss: 0.947 |  Val. Acc: 68.84%\n",
      "Epoch: 12 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.962 | Train Acc: 69.56%\n",
      "\t Val. Loss: 0.897 |  Val. Acc: 70.19%\n",
      "Epoch: 13 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.911 | Train Acc: 71.06%\n",
      "\t Val. Loss: 0.848 |  Val. Acc: 71.71%\n",
      "Epoch: 14 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.866 | Train Acc: 72.78%\n",
      "\t Val. Loss: 0.808 |  Val. Acc: 73.04%\n",
      "Epoch: 15 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.829 | Train Acc: 73.69%\n",
      "\t Val. Loss: 0.768 |  Val. Acc: 74.33%\n",
      "Epoch: 16 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.790 | Train Acc: 75.07%\n",
      "\t Val. Loss: 0.735 |  Val. Acc: 75.32%\n",
      "Epoch: 17 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.758 | Train Acc: 76.18%\n",
      "\t Val. Loss: 0.701 |  Val. Acc: 76.92%\n",
      "Epoch: 18 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.729 | Train Acc: 77.08%\n",
      "\t Val. Loss: 0.672 |  Val. Acc: 77.93%\n",
      "Epoch: 19 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.705 | Train Acc: 77.82%\n",
      "\t Val. Loss: 0.646 |  Val. Acc: 78.78%\n",
      "Epoch: 20 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.681 | Train Acc: 78.71%\n",
      "\t Val. Loss: 0.623 |  Val. Acc: 79.70%\n",
      "Epoch: 21 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.656 | Train Acc: 79.26%\n",
      "\t Val. Loss: 0.607 |  Val. Acc: 80.63%\n",
      "Epoch: 22 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.642 | Train Acc: 79.85%\n",
      "\t Val. Loss: 0.585 |  Val. Acc: 81.39%\n"
     ]
    }
   ],
   "source": [
    "class BiLSTMPOSTagger(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim, \n",
    "                 n_layers, \n",
    "                 bidirectional, \n",
    "                 dropout, \n",
    "                 pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(word2vec_weights)\n",
    "        #self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            num_layers = n_layers, \n",
    "                            bidirectional = bidirectional,\n",
    "                            dropout = dropout if n_layers > 1 else 0)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        #pass text through embedding layer\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        #pass embeddings into LSTM\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        #outputs holds the backward and forward hidden states in the final layer\n",
    "        #hidden and cell are the backward and forward hidden and cell states at the final time-step\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim * n directions]\n",
    "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #we use our outputs to make a prediction of what the tag should be\n",
    "        predictions = self.fc(self.dropout(outputs))\n",
    "        \n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean = 0, std = 0.1)\n",
    "        \n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def categorical_accuracy(preds, y, tag_pad_idx):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
    "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
    "    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]]).to(device)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, tag_pad_idx):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    count = 0\n",
    "    for x, y in iterator:\n",
    "        count += 1\n",
    "        text = x.reshape(x.shape[1], x.shape[0]).to(device)\n",
    "        tags = y.reshape(x.shape[1], x.shape[0]).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        predictions = model(text)\n",
    "        \n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "        #tags = [sent len, batch size]\n",
    "        \n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "        \n",
    "        #predictions = [sent len * batch size, output dim]\n",
    "        #tags = [sent len * batch size]\n",
    "        \n",
    "        loss = criterion(predictions, tags)\n",
    "                \n",
    "        acc = categorical_accuracy(predictions.to(device), tags, tag_pad_idx)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / count, epoch_acc / count\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion, tag_pad_idx):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for x, y in iterator:\n",
    "            count += 1\n",
    "            text = x.reshape(x.shape[1], x.shape[0]).to(device)\n",
    "            tags = y.reshape(x.shape[1], x.shape[0]).to(device)\n",
    "            \n",
    "            predictions = model(text)\n",
    "            \n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "            \n",
    "            loss = criterion(predictions, tags)\n",
    "            \n",
    "            acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / count, epoch_acc / count\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "\n",
    "vocab_words, vocab_tags = make_vocabs(train_data)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 200\n",
    "INPUT_DIM = word2vec_weights.shape[0]\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 220\n",
    "OUTPUT_DIM = len(vocab_tags)\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.2\n",
    "PAD_IDX = 0\n",
    "TAG_PAD_IDX = 0\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "model = BiLSTMPOSTagger(INPUT_DIM, \n",
    "                        EMBEDDING_DIM, \n",
    "                        HIDDEN_DIM, \n",
    "                        OUTPUT_DIM, \n",
    "                        N_LAYERS, \n",
    "                        BIDIRECTIONAL, \n",
    "                        DROPOUT, \n",
    "                        PAD_IDX)\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_iterator = training_examples_tagger2(vocab_words, vocab_tags, train_data, BATCH_SIZE)\n",
    "    valid_iterator= training_examples_tagger2(vocab_words, vocab_tags, dev_data, BATCH_SIZE)\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, TAG_PAD_IDX)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, TAG_PAD_IDX)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(word2vec_model.get_index('Adam'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.save('test_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
