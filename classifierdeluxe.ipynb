{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency parsing baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dependency parsing is the task of mapping a sentence to a formal representation of its syntactic structure in the form of a dependency tree, which consists of directed arcs between individual words (tokens). Here we will implement a dependency parser baseline based on the arc-standard algorithm and the fixed-window model that we implemented in Lab L3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batchify import *\n",
    "from create_vocab import *\n",
    "from data_handling import *\n",
    "from parser import *\n",
    "from projectivize import *\n",
    "from uas import *\n",
    "from window_models import *\n",
    "from taggers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset('./data/en_gum-ud-train-projectivized.conllu')\n",
    "dev_data = Dataset('./data/en_gum-ud-dev-projectivized.conllu')\n",
    "test_data = Dataset('./data/en_gum-ud-test-projectivized.conllu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train_fixed_window(train_data, n_epochs=2, batch_size=100, lr=1e-2):\n",
    "    vocab_words, vocab_tags = make_vocabs(train_data)\n",
    "    tagger = FixedWindowTagger(vocab_words, vocab_tags, len(vocab_tags))\n",
    "    \n",
    "    optimizer = optim.Adam(tagger.model.parameters(), lr=lr)\n",
    "    for i in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        batch_nr = 0\n",
    "        for x, y in training_examples_tagger(vocab_words, vocab_tags, train_data, tagger):\n",
    "            batch_nr += 1\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_pred = tagger.model.forward(x)\n",
    "            \n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            if batch_nr % 100 == 1:\n",
    "                print(total_loss/batch_nr)\n",
    "                #pass\n",
    "    return tagger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.988539218902588\n",
      "1.0766507263230805\n",
      "0.7761470550921425\n",
      "0.6433389592556858\n",
      "0.5670698325049848\n",
      "0.5218889229222686\n",
      "0.4924586370165852\n",
      "0.4737867672798807\n",
      "0.4521804275491116\n",
      "0.41529327630996704\n",
      "0.1878917758712674\n",
      "0.14903390525020102\n",
      "0.13142143503803005\n",
      "0.12925835619994122\n",
      "0.1302114050814923\n",
      "0.12821916359462798\n",
      "0.12605362650360408\n",
      "0.1250453132644212\n"
     ]
    }
   ],
   "source": [
    "tagger = train_fixed_window(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8818215114149677"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(tagger, dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "def train_fixed_parser(train_data, n_epochs=2, batch_size=100, lr=1e-2):\n",
    "    vocab_words, vocab_tags = make_vocabs(train_data)\n",
    "    parser = FixedWindowParser(vocab_words, vocab_tags)\n",
    "    \n",
    "    optimizer = optim.Adam(parser.model.parameters(), lr=lr)\n",
    "    for i in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        batch_nr = 0\n",
    "        for x, y in training_examples_parser(vocab_words, vocab_tags, train_data, parser):\n",
    "            batch_nr += 1\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_pred = parser.model.forward(x)\n",
    "            \n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            if batch_nr % 100 == 1:\n",
    "                print(total_loss/batch_nr)\n",
    "                \n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3709524869918823\n",
      "0.67494798414778\n",
      "0.5991728283576111\n",
      "0.5571447691549098\n",
      "0.5313198175513536\n",
      "0.5055585305788322\n",
      "0.488999368991709\n",
      "0.4782408505423093\n",
      "0.46919370694702184\n",
      "0.4620046924373815\n",
      "0.4573909454531484\n",
      "0.4526904279215568\n",
      "0.44704477655872715\n",
      "0.4444693769747069\n",
      "0.4410749453676334\n",
      "0.4351727965670693\n",
      "0.42888487999715486\n",
      "0.423105035658306\n",
      "0.41803419975556644\n",
      "0.41540059313640537\n",
      "0.4107631054231669\n",
      "0.4052849263271963\n",
      "0.4013678312812784\n",
      "0.4001972087291027\n",
      "0.40037046614468025\n",
      "0.39861359635367244\n",
      "0.3970938696497704\n",
      "0.3954218155416525\n",
      "0.39414156328451844\n",
      "0.3923030519901021\n",
      "0.39210728127279704\n",
      "0.39249709338041056\n",
      "0.3921419018958089\n",
      "0.39205022081868174\n",
      "0.3923011830821962\n",
      "0.3917929880103957\n",
      "0.3921838573412129\n",
      "0.3929796988299024\n",
      "0.3916039378581957\n",
      "0.3917746496145938\n",
      "0.39148327652099607\n",
      "0.3905888292379814\n",
      "0.38949928266080297\n",
      "0.38854517761450735\n",
      "0.38832963411492044\n",
      "0.3867926941412776\n",
      "0.38577666895784285\n",
      "0.44775471091270447\n",
      "0.35572939476754406\n",
      "0.3588000005157433\n",
      "0.35731272523189306\n",
      "0.3537902399303015\n",
      "0.3457446013709266\n",
      "0.3420526046498048\n",
      "0.3392461152606446\n",
      "0.33627187894115435\n",
      "0.3359291539853077\n",
      "0.3358094447127708\n",
      "0.33548997958392907\n",
      "0.33341950075165816\n",
      "0.33359258670608966\n",
      "0.3332435586818791\n",
      "0.32973989494378214\n",
      "0.32611717776553023\n",
      "0.3231040364386473\n",
      "0.32024303678503835\n",
      "0.3196997298873267\n",
      "0.31757258266016936\n",
      "0.31416650138784735\n",
      "0.31240983185329146\n",
      "0.31266699228654316\n",
      "0.3136423007848485\n",
      "0.313180863873654\n",
      "0.31308722717602266\n",
      "0.3122069796203177\n",
      "0.3117569695582287\n",
      "0.3114989761074364\n",
      "0.31185765097818346\n",
      "0.3122660726445773\n",
      "0.31252321051256204\n",
      "0.3127530344796611\n",
      "0.3136282470159428\n",
      "0.31339504117647843\n",
      "0.313873522048798\n",
      "0.3148803868253258\n",
      "0.3139331498337918\n",
      "0.3143365995992348\n",
      "0.3143468104112941\n",
      "0.31417931582241837\n",
      "0.31356271862065016\n",
      "0.3131917056949561\n",
      "0.3135687899148361\n",
      "0.3126362612128748\n",
      "0.3126259275522408\n"
     ]
    }
   ],
   "source": [
    "parser = train_fixed_parser(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anton/git/nlp-project/parser.py:143: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = torch.nn.functional.log_softmax(pred)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7486216181561739"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uas(parser, dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<root>', '<root>', 0), ('However', 'ADV', 6), (',', 'PUNCT', 1), ('it', 'PRON', 6), ('is', 'AUX', 6), ('not', 'PART', 6), ('enough', 'ADJ', 0), ('to', 'PART', 9), ('have', 'AUX', 9), ('attained', 'VERB', 6), ('such', 'ADJ', 12), ('native-like', 'ADJ', 12), ('levels', 'NOUN', 9), ('.', 'PUNCT', 6)]\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dev_data):\n",
    "    if i == 3:\n",
    "        uas(parser, [data])\n",
    "        print(data)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing parser with predicted tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_uas_with_tagger_preds(tagger, parser, data):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    new_data = []\n",
    "    for sent in data:\n",
    "        pred_tags = tagger.predict(sent)\n",
    "    \n",
    "        # Replace gold tags with predicted\n",
    "        for i , (_, tag) in enumerate(pred_tags):\n",
    "            sent[i] = (sent[i][0], tag, sent[i][2])\n",
    "        new_data.append(sent)\n",
    "        \n",
    "    return uas(parser, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6929093473522246"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_uas_with_tagger_preds(tagger, parser, dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
