{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gensim.models import KeyedVectors\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import torchtext\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import random\n",
    "from data_handling import *\n",
    "\n",
    "from create_vocab import *\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset('data/en_gum-ud-train-projectivized.conllu')\n",
    "dev_data = Dataset('data/en_gum-ud-dev-projectivized.conllu')\n",
    "test_data = Dataset('data/en_gum-ud-test-projectivized.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#filename='GoogleNews-vectors-negative300.bin' ; binary = False ; no_header = False# ~ 80% accuracy\n",
    "\n",
    "#filename = 'enwiki_20180420_win10_100d.txt' ; binary = False  ; no_header = False # 85% accuracy\n",
    "#filename = 'enwiki_20180420_win10_300d.txt' ; binary = False ; no_header = False\n",
    "#filename = 'enwiki_20180420_win10_500d.txt' ; binary = False ; no_header = False\n",
    "#filename = 'enwiki_20180420_nolg_300d.txt' ; binary = False ; no_header = False\n",
    "#filename = 'enwiki_20180420_nolg_500d.txt' ; binary = False ; no_header = False\n",
    "\n",
    "#wikipedia + gigaword 5\n",
    "#filename = 'glove.6B.50d.txt' ; binary = False ; no_header = True\n",
    "#filename = 'glove.6B.100d.txt' ; binary = False ; no_header = True  # 89.21% \n",
    "#filename = 'glove.6B.200d.txt' ; binary = False ; no_header = True # ~88.4% small hidden size\n",
    "#filename = 'glove.6B.300d.txt' ; binary = False ; no_header = True\n",
    "\n",
    "\n",
    "# Twitter embeddings\n",
    "#filename = 'glove.twitter.27B.25d.txt' ; binary = False ; no_header = True\n",
    "#filename = 'glove.twitter.27B.50d.txt' ; binary = False ; no_header = True\n",
    "#filename = 'glove.twitter.27B.100d.txt' ; binary = False ; no_header = True\n",
    "#filename = 'glove.twitter.27B.200d.txt' ; binary = False ; no_header = True\n",
    "\n",
    "\n",
    "\n",
    "filename = 'glove.840B.300d.txt' ; binary = False ; no_header = True\n",
    "\n",
    " \n",
    "#word2vec_model = KeyedVectors.load_word2vec_format(filename, binary=binary, no_header=no_header)\n",
    "word2vec_model = KeyedVectors.load('embedding_weights')\n",
    "word2vec_weights = torch.FloatTensor(word2vec_model.vectors)\n",
    "\n",
    "def training_examples_tagger2(vocab_words, vocab_tags, gold_data, batch_size=100, max_len=40):\n",
    "    assert batch_size > 0 and max_len > 0\n",
    "        \n",
    "    # max sequence length\n",
    "    x = torch.zeros((batch_size, max_len)).long()\n",
    "    x[:,:] = 2196016 \n",
    "    y = torch.zeros((batch_size, max_len)).long()\n",
    "    count = 0\n",
    "    for sentence in gold_data:\n",
    "        # Remove <root> element\n",
    "        sentence = [(\"@\", elem[1], elem[2]) if elem[0] == \"<root>\" else elem for elem in sentence]\n",
    "        words = torch.Tensor(list(map(lambda x: word2vec_model.get_index(x[0]) if x[0] in word2vec_model else 2196015 , sentence))).long()\n",
    "        labels = torch.Tensor(list(map(lambda x: vocab_tags[x[1]], sentence))).long()\n",
    "        # pad to max len\n",
    "        x[count,:len(words)] = words[:max_len]\n",
    "        \n",
    "        y[count,:len(labels)] = labels[:max_len]\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        if(count == batch_size):\n",
    "            yield x.long() ,y.long()\n",
    "            x = torch.zeros((batch_size, max_len))\n",
    "            y = torch.zeros((batch_size, max_len))\n",
    "            count = 0\n",
    "    if(count):\n",
    "        yield x[:count,:].long(), y[:count,:].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_words, vocab_tags = make_vocabs(train_data)\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 200\n",
    "INPUT_DIM = word2vec_weights.shape[0]\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 200\n",
    "OUTPUT_DIM = len(vocab_tags)\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "PAD_IDX = 2196016\n",
    "TAG_PAD_IDX = 0\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(word2vec_weights, padding_idx=2196016)#, freeze=True)\n",
    "        #self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            num_layers = n_layers, \n",
    "                            bidirectional = bidirectional,\n",
    "                            dropout = dropout\n",
    "                           )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear1 = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        res = self.linear1(self.dropout(lstm_out))\n",
    "        \n",
    "        \n",
    "        return res\n",
    "    \n",
    "\n",
    "# accurayc per batch\n",
    "def tagger_accuracy(preds, y, tag_pad_idx):\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) \n",
    "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
    "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
    "    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]]).to(device)\n",
    "\n",
    "\n",
    "def train_tagger():\n",
    "    \n",
    "\n",
    "    model = LSTMTagger(INPUT_DIM, \n",
    "                            EMBEDDING_DIM, \n",
    "                            HIDDEN_DIM, \n",
    "                            OUTPUT_DIM, \n",
    "                            N_LAYERS, \n",
    "                            BIDIRECTIONAL, \n",
    "                            DROPOUT, \n",
    "                            PAD_IDX)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "\n",
    "    \n",
    "    \n",
    "    best_valid_acc = 0\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        model.train()\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_iterator = training_examples_tagger2(vocab_words, vocab_tags, train_data, BATCH_SIZE)\n",
    "        #valid_iterator= training_examples_tagger2(vocab_words, vocab_tags, dev_data, BATCH_SIZE)\n",
    "\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        model.train()\n",
    "        count = 0\n",
    "        for x, y in train_iterator:\n",
    "            count += 1\n",
    "            text = x.reshape(x.shape[1], x.shape[0]).to(device)\n",
    "            tags = y.reshape(x.shape[1], x.shape[0]).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions = model.forward(text)\n",
    "\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "\n",
    "            loss = criterion(predictions, tags)\n",
    "\n",
    "            acc = tagger_accuracy(predictions.to(device), tags, TAG_PAD_IDX)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_acc += acc.item()\n",
    "\n",
    "        train_loss /= count\n",
    "        train_acc /= count\n",
    "        valid_loss, valid_acc = evaluate(model, criterion, TAG_PAD_IDX, dev_data)\n",
    "\n",
    "        if valid_acc > best_valid_acc and valid_acc > 0.91:\n",
    "            best_valid_acc = valid_acc\n",
    "            torch.save(model.state_dict(), 'best-tagger-model.pt')\n",
    "            \n",
    "        print(f'Epoch: {epoch+1}')\n",
    "        print(f'Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.3f}%')\n",
    "        print(f'Val. Loss: {valid_loss:.3f},  Val. Acc: {valid_acc*100:.3f}%')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_saved_tagger(path=\"best-tagger-model.pt\"):\n",
    "    #model = LSTMTagger(*args, **kwargs)\n",
    "    model = LSTMTagger(INPUT_DIM, \n",
    "                            EMBEDDING_DIM, \n",
    "                            HIDDEN_DIM, \n",
    "                            OUTPUT_DIM, \n",
    "                            N_LAYERS, \n",
    "                            BIDIRECTIONAL, \n",
    "                            DROPOUT, \n",
    "                            PAD_IDX)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    return model\n",
    "    \n",
    "\n",
    "def evaluate(model, criterion, tag_pad_idx, data):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    count = 0\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = tag_pad_idx)\n",
    "\n",
    "    criterion = criterion.to(device)\n",
    "    \n",
    "    data = training_examples_tagger2(vocab_words, vocab_tags, data, BATCH_SIZE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for x, y in data:\n",
    "            count += 1\n",
    "            text = x.reshape(x.shape[1], x.shape[0]).to(device)\n",
    "            tags = y.reshape(x.shape[1], x.shape[0]).to(device)\n",
    "            \n",
    "            predictions = model(text)\n",
    "            \n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "            #print(predictions)\n",
    "            \n",
    "            loss = criterion(predictions, tags)\n",
    "            \n",
    "            acc = tagger_accuracy(predictions, tags, tag_pad_idx)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / count, epoch_acc / count\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = False\n",
    "if train_model:\n",
    "    model = train_tagger()\n",
    "else:\n",
    "    model = get_saved_tagger()\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<root>',\n",
       " 'PUNCT',\n",
       " 'PRON',\n",
       " 'NOUN',\n",
       " 'AUX',\n",
       " 'PROPN',\n",
       " 'CCONJ',\n",
       " 'AUX',\n",
       " 'ADJ',\n",
       " 'PUNCT',\n",
       " 'CCONJ',\n",
       " 'PART',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'PUNCT']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def predict(model, sentence, vocab_tags):\n",
    "    id2tag = list(vocab_tags.keys())\n",
    "    words = [word for word in sentence]\n",
    "    encoded = torch.LongTensor([word2vec_model.get_index(word) for word in words]).to(device)\n",
    "    res = torch.argmax(model(encoded.unsqueeze(dim=0)), dim=2)\n",
    "    return [id2tag[x] for x in res.squeeze()]\n",
    "\n",
    "predict(model, \"@ Hello my name is Benjamin and I'm large , but not in height .\".split(), vocab_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.491 %\n",
      "90.674 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "valid_loss, valid_acc = evaluate(model, criterion, TAG_PAD_IDX, dev_data)\n",
    "print(f\"{valid_acc*100:.3f} %\")\n",
    "test_loss, test_acc = evaluate(model, criterion, TAG_PAD_IDX, test_data)\n",
    "print(f\"{test_acc*100:.3f} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(word2vec_model[2196015])\n",
    "\n",
    "#word2vec_model[\"@\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-cf6a3503e660>:1: DeprecationWarning: Call to deprecated `init_sims` (Use fill_norms() instead. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  word2vec_model.init_sims(replace=True)\n",
      "/home/benjamin/.local/lib/python3.8/site-packages/gensim/models/keyedvectors.py:1470: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.vectors /= self.norms[..., np.newaxis]\n"
     ]
    }
   ],
   "source": [
    "#word2vec_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.save('embedding_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
