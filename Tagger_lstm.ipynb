{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gensim.models import KeyedVectors\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import torchtext\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import random\n",
    "from data_handling import *\n",
    "\n",
    "from create_vocab import *\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset('data/en_gum-ud-train-projectivized.conllu')\n",
    "dev_data = Dataset('data/en_gum-ud-dev-projectivized.conllu')\n",
    "test_data = Dataset('data/en_gum-ud-test-projectivized.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#filename='GoogleNews-vectors-negative300.bin' ; binary = False ; no_header = False# ~ 80% accuracy\n",
    "\n",
    "#filename = 'enwiki_20180420_win10_100d.txt' ; binary = False  ; no_header = False # 85% accuracy\n",
    "#filename = 'enwiki_20180420_win10_300d.txt' ; binary = False ; no_header = False\n",
    "#filename = 'enwiki_20180420_win10_500d.txt' ; binary = False ; no_header = False\n",
    "#filename = 'enwiki_20180420_nolg_300d.txt' ; binary = False ; no_header = False\n",
    "#filename = 'enwiki_20180420_nolg_500d.txt' ; binary = False ; no_header = False\n",
    "\n",
    "#wikipedia + gigaword 5\n",
    "#filename = 'glove.6B.50d.txt' ; binary = False ; no_header = True\n",
    "#filename = 'glove.6B.100d.txt' ; binary = False ; no_header = True  # 89.21% \n",
    "#filename = 'glove.6B.200d.txt' ; binary = False ; no_header = True # ~88.4% small hidden size\n",
    "#filename = 'glove.6B.300d.txt' ; binary = False ; no_header = True\n",
    "\n",
    "\n",
    "# Twitter embeddings\n",
    "#filename = 'glove.twitter.27B.25d.txt' ; binary = False ; no_header = True\n",
    "#filename = 'glove.twitter.27B.50d.txt' ; binary = False ; no_header = True\n",
    "#filename = 'glove.twitter.27B.100d.txt' ; binary = False ; no_header = True\n",
    "#filename = 'glove.twitter.27B.200d.txt' ; binary = False ; no_header = True\n",
    "\n",
    "\n",
    "\n",
    "filename = 'glove.840B.300d.txt' ; binary = False ; no_header = True\n",
    "\n",
    " \n",
    "#word2vec_model = KeyedVectors.load_word2vec_format(filename, binary=binary, no_header=no_header)\n",
    "word2vec_model = KeyedVectors.load('embedding_weights')\n",
    "word2vec_weights = torch.FloatTensor(word2vec_model.vectors)\n",
    "\n",
    "def training_examples_tagger2(vocab_words, vocab_tags, gold_data, batch_size=100, max_len=40):\n",
    "    assert batch_size > 0 and max_len > 0\n",
    "        \n",
    "    # max sequence length\n",
    "    x = torch.zeros((batch_size, max_len)).long()\n",
    "    x[:,:] = 2196016 \n",
    "    y = torch.zeros((batch_size, max_len)).long()\n",
    "    count = 0\n",
    "    for sentence in gold_data:\n",
    "        words = torch.Tensor(list(map(lambda x: word2vec_model.get_index(x[0]) if x[0] in word2vec_model else 2196015 , sentence))).long()\n",
    "        \n",
    "        labels = torch.Tensor(list(map(lambda x: vocab_tags[x[1]], sentence))).long()\n",
    "        # pad to max len\n",
    "        x[count,:len(words)] = words[:max_len]\n",
    "        \n",
    "        y[count,:len(labels)] = labels[:max_len]\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        if(count == batch_size):\n",
    "            yield x.long() ,y.long()\n",
    "            x = torch.zeros((batch_size, max_len))\n",
    "            y = torch.zeros((batch_size, max_len))\n",
    "            count = 0\n",
    "    if(count):\n",
    "        yield x[:count,:].long(), y[:count,:].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_words, vocab_tags = make_vocabs(train_data)\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 200\n",
    "INPUT_DIM = word2vec_weights.shape[0]\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 200\n",
    "OUTPUT_DIM = len(vocab_tags)\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "PAD_IDX = 2196016\n",
    "TAG_PAD_IDX = 0\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(word2vec_weights, padding_idx=2196016)#, freeze=True)\n",
    "        #self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            num_layers = n_layers, \n",
    "                            bidirectional = bidirectional,\n",
    "                            dropout = dropout\n",
    "                           )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear1 = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        res = self.linear1(self.dropout(lstm_out))\n",
    "        \n",
    "        \n",
    "        return res\n",
    "    \n",
    "\n",
    "# accurayc per batch\n",
    "def tagger_accuracy(preds, y, tag_pad_idx):\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) \n",
    "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
    "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
    "    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]]).to(device)\n",
    "\n",
    "\n",
    "def train_tagger():\n",
    "    \n",
    "\n",
    "    model = LSTMTagger(INPUT_DIM, \n",
    "                            EMBEDDING_DIM, \n",
    "                            HIDDEN_DIM, \n",
    "                            OUTPUT_DIM, \n",
    "                            N_LAYERS, \n",
    "                            BIDIRECTIONAL, \n",
    "                            DROPOUT, \n",
    "                            PAD_IDX)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "\n",
    "    \n",
    "    \n",
    "    best_valid_acc = 0\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        model.train()\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_iterator = training_examples_tagger2(vocab_words, vocab_tags, train_data, BATCH_SIZE)\n",
    "        #valid_iterator= training_examples_tagger2(vocab_words, vocab_tags, dev_data, BATCH_SIZE)\n",
    "\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        model.train()\n",
    "        count = 0\n",
    "        for x, y in train_iterator:\n",
    "            count += 1\n",
    "            text = x.reshape(x.shape[1], x.shape[0]).to(device)\n",
    "            tags = y.reshape(x.shape[1], x.shape[0]).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions = model.forward(text)\n",
    "\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "\n",
    "            loss = criterion(predictions, tags)\n",
    "\n",
    "            acc = tagger_accuracy(predictions.to(device), tags, TAG_PAD_IDX)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_acc += acc.item()\n",
    "\n",
    "        train_loss /= count\n",
    "        train_acc /= count\n",
    "        valid_loss, valid_acc = evaluate(model, criterion, TAG_PAD_IDX, dev_data)\n",
    "\n",
    "        if valid_acc > best_valid_acc and valid_acc > 0.91:\n",
    "            best_valid_acc = valid_acc\n",
    "            torch.save(model.state_dict(), 'best-tagger-model.pt')\n",
    "            \n",
    "        print(f'Epoch: {epoch+1}')\n",
    "        print(f'Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.3f}%')\n",
    "        print(f'Val. Loss: {valid_loss:.3f},  Val. Acc: {valid_acc*100:.3f}%')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_saved_tagger(path=\"best-tagger-model.pt\"):\n",
    "    #model = LSTMTagger(*args, **kwargs)\n",
    "    model = LSTMTagger(INPUT_DIM, \n",
    "                            EMBEDDING_DIM, \n",
    "                            HIDDEN_DIM, \n",
    "                            OUTPUT_DIM, \n",
    "                            N_LAYERS, \n",
    "                            BIDIRECTIONAL, \n",
    "                            DROPOUT, \n",
    "                            PAD_IDX)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    return model\n",
    "    \n",
    "\n",
    "def evaluate(model, criterion, tag_pad_idx, data):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    count = 0\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = tag_pad_idx)\n",
    "\n",
    "    criterion = criterion.to(device)\n",
    "    \n",
    "    data = training_examples_tagger2(vocab_words, vocab_tags, data, BATCH_SIZE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for x, y in data:\n",
    "            count += 1\n",
    "            text = x.reshape(x.shape[1], x.shape[0]).to(device)\n",
    "            tags = y.reshape(x.shape[1], x.shape[0]).to(device)\n",
    "            \n",
    "            predictions = model(text)\n",
    "            \n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "            \n",
    "            loss = criterion(predictions, tags)\n",
    "            \n",
    "            acc = tagger_accuracy(predictions, tags, tag_pad_idx)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / count, epoch_acc / count\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = train_tagger()\n",
    "model = get_saved_tagger()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INTJ',\n",
       " 'PRON',\n",
       " 'NOUN',\n",
       " 'AUX',\n",
       " 'PROPN',\n",
       " 'CCONJ',\n",
       " 'PRON',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'ADV',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'PUNCT']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_words, vocab_tags = make_vocabs(train_data)\n",
    "\n",
    "def predict(model, sentence, vocab_tags):\n",
    "    id2tag = list(vocab_tags.keys())\n",
    "    words = [word for word in sentence]\n",
    "    encoded = torch.LongTensor([word2vec_model.get_index(word) for word in words]).to(device)\n",
    "    res = torch.argmax(model(encoded.unsqueeze(dim=0)), dim=2)\n",
    "    return [id2tag[x] for x in res.squeeze()]\n",
    "\n",
    "predict(model, \"hello my name is Benjamin and I have a very big penis .\".split(), vocab_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.491 %\n",
      "90.674 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "valid_loss, valid_acc = evaluate(model, criterion, TAG_PAD_IDX, dev_data)\n",
    "print(f\"{valid_acc*100:.3f} %\")\n",
    "test_loss, test_acc = evaluate(model, criterion, TAG_PAD_IDX, test_data)\n",
    "print(f\"{test_acc*100:.3f} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.9690e-02 -2.2905e-01  8.0366e-01 -7.8865e-01 -4.0567e-01 -1.5716e-01\n",
      " -4.2302e-01  6.4081e-01 -1.3215e-01 -1.4109e+00  7.3118e-01 -3.7391e-01\n",
      " -3.6422e-01  2.4199e-02 -2.4359e-01  1.0140e+00  6.5176e-04 -8.9537e-01\n",
      "  8.0540e-01 -7.3101e-02  2.0257e-01  5.9553e-01 -3.4971e-03 -2.8126e-01\n",
      "  5.8631e-01 -1.7115e-01  1.2428e-01  5.3392e-01  4.8289e-01  3.6989e-01\n",
      " -9.1151e-02 -2.3874e-01  3.8864e-01 -1.6403e-01 -8.5745e-01  1.9000e-01\n",
      "  4.1450e-01  3.5958e-01 -1.8726e-02  5.5213e-01 -9.1331e-03 -4.8204e-01\n",
      " -6.4685e-01  6.1736e-01 -2.7128e-01  1.3459e-01  9.4729e-01 -4.2939e-01\n",
      " -3.2462e-01 -8.8466e-02  3.7337e-01  2.9062e-01 -7.4411e-03  1.9840e-01\n",
      " -4.2686e-01 -7.1294e-02 -4.3443e-02 -3.3026e-03 -1.0519e-01  2.0885e-01\n",
      " -3.0217e-01  2.7366e-01 -3.5602e-01 -8.9143e-01  2.8561e-01 -1.1656e-01\n",
      "  2.2460e-01 -2.1561e-02 -1.6219e-02 -9.6267e-01  8.5239e-01 -1.2714e+00\n",
      " -8.4290e-01  2.5947e-01  1.0074e-01 -1.2530e-01  1.6124e-02  1.2488e-01\n",
      "  1.6413e-01 -4.6028e-01  3.2825e-01 -5.1367e-01 -1.6456e-01  5.6410e-01\n",
      "  9.2562e-02  1.1196e+00  1.5936e-01  6.6175e-01 -1.0068e+00 -8.6162e-02\n",
      "  1.7847e-01 -4.6644e-01  1.2672e-01  3.1786e-01 -2.5533e-01  7.3502e-01\n",
      " -1.0719e-02  5.4341e-03 -1.3019e-02  6.0229e-01  6.1846e-02  6.1026e-02\n",
      "  7.5747e-01  6.8770e-01  2.6887e-02 -3.6918e-01 -1.7628e-01 -7.7614e-01\n",
      " -6.9935e-01  5.3906e-01  9.7763e-02 -2.3648e-01 -2.3217e-01 -3.5618e-01\n",
      "  4.9942e-02 -4.8307e-02 -7.1276e-01 -8.0498e-01 -4.9700e-03 -1.4976e-01\n",
      "  5.1274e-01 -3.0659e-01  1.2332e-01  4.6295e-01  5.1516e-01 -1.1373e+00\n",
      " -5.7126e-01  5.1350e-01  2.9104e-01 -8.6347e-01  4.4613e-01 -8.1658e-01\n",
      " -2.9672e-01 -7.1397e-01 -3.3071e-01 -1.2573e-01 -1.6253e-01  3.1273e-01\n",
      " -5.9367e-01 -3.3150e-02  1.2405e+00  2.6456e-01  1.0989e-01 -3.3882e-01\n",
      "  2.6638e-01  4.9057e-02 -3.6959e-01 -4.0592e-01 -2.2758e-01  6.0450e-01\n",
      " -3.7629e-01  2.4219e-01  2.7733e-01 -6.3102e-01  2.6867e-01  7.7098e-02\n",
      "  4.1419e-01 -1.2265e-01 -1.0442e-01  6.5540e-01 -2.8348e-01  1.6120e-02\n",
      " -1.1086e-01  3.8989e-01  9.8121e-01  1.3837e+00 -4.8673e-01  1.9253e-01\n",
      " -8.3225e-01 -6.1103e-01 -1.3101e-01  1.0166e-02  2.1825e-01  8.0634e-01\n",
      "  4.8111e-01 -3.1594e-01  1.0402e-01 -6.0965e-01 -4.2259e-01 -2.6895e-01\n",
      " -4.3522e-01  6.2865e-01  9.1043e-02 -5.9981e-02  2.8502e-01 -3.1621e-01\n",
      "  3.6937e-02  1.3772e-01 -3.0015e-01  3.7415e-01  3.3253e-01 -1.4745e-01\n",
      "  1.4210e-01 -4.1723e-01 -8.8119e-02 -5.2391e-01  1.9749e-02  1.0092e+00\n",
      "  6.7470e-01 -7.0984e-01 -4.4131e-01  1.9539e-02 -1.3180e-01 -1.1064e-01\n",
      " -5.7117e-01  1.4544e-01 -5.4714e-01 -2.4794e-01  6.4769e-01  8.3943e-02\n",
      "  6.7539e-01  6.3883e-01  9.7277e-04  7.0608e-01 -1.9377e-01 -5.9744e-01\n",
      "  4.8640e-01 -5.6739e-02  2.6321e-01  6.7880e-02 -1.2797e-01  7.2588e-02\n",
      " -2.0607e-01  2.8131e-01  8.0015e-01 -4.6605e-01  1.9531e-01 -3.6227e-01\n",
      " -2.0949e-01 -1.1307e-01  4.2540e-01  1.0286e-01 -2.8195e-01 -1.1570e-01\n",
      "  4.7033e-01  3.1709e-01  3.3137e-01 -1.4531e-01 -1.0918e-01 -2.3374e-01\n",
      " -1.9897e-01  1.2693e-01 -3.7470e-02 -4.5309e-01 -3.4972e-01 -3.3820e-01\n",
      "  7.0407e-01 -1.0550e-01  7.8543e-01  3.4963e-02 -6.8759e-01  7.4536e-01\n",
      "  2.5425e-02 -2.0919e-01 -2.2691e-03 -9.0022e-01 -7.1534e-01  6.5250e-01\n",
      " -1.0571e-01 -4.9702e-01 -3.8476e-01 -3.9235e-01  1.1160e-01 -2.0217e-01\n",
      " -4.3163e-01  4.2698e-01  2.0545e-01  4.0360e-01 -8.6946e-01  5.7366e-01\n",
      " -1.3683e-01  6.5796e-01  6.1283e-01  2.7316e-01 -7.3551e-01 -7.0123e-01\n",
      " -3.9056e-01 -4.3813e-01 -3.2104e-01 -6.1864e-01 -7.4312e-01 -4.9329e-01\n",
      " -7.0878e-01 -3.5697e-01  7.9095e-01  6.2299e-01 -3.6023e-01  6.6178e-01\n",
      " -5.4589e-01  1.0604e-01  6.4657e-01 -4.1591e-01  1.4240e-01 -5.1749e-02\n",
      "  3.8925e-01 -2.0522e-01  2.6878e-01 -8.3561e-02  4.8532e-01 -7.3130e-01]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(word2vec_model[2196015])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-cf6a3503e660>:1: DeprecationWarning: Call to deprecated `init_sims` (Use fill_norms() instead. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  word2vec_model.init_sims(replace=True)\n",
      "/home/benjamin/.local/lib/python3.8/site-packages/gensim/models/keyedvectors.py:1470: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.vectors /= self.norms[..., np.newaxis]\n"
     ]
    }
   ],
   "source": [
    "#word2vec_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.save('embedding_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
